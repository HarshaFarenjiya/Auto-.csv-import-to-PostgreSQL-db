{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020250a4-70eb-457e-b921-29621eb3e7ea",
   "metadata": {},
   "source": [
    "# Automated .csv file import into a PostgreSQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b829f-08f5-4452-b1a2-3452ff4335ea",
   "metadata": {},
   "source": [
    "## Introduction :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd39c4d-d71c-4d43-b6ea-612b36948fdf",
   "metadata": {},
   "source": [
    "### In PostgreSQL, it is necessary to specify the data types of columns before importing data. This can be more cumbersome, especially when dealing with tables with a large number of columns. The requirement to define the data types in advance can make the import process more time-consuming and error-prone, as it involves accurately specifying the data types for each column.\n",
    "### However, it's important to note that PostgreSQL's approach of explicitly defining data types offers the advantage of data integrity enforcement and stricter control over the data being imported. It ensures that the imported data aligns with the defined table schema, reducing the risk of data inconsistencies or integrity violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c207f548-f445-4765-9095-45bcafed5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e976e2-b218-4b0e-8f11-6c405f632fc7",
   "metadata": {},
   "source": [
    "h = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb0f400a-9ed4-4501-bf2d-8e476d0d303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.condarc',\n",
       " '.ipynb_checkpoints',\n",
       " '.ipython',\n",
       " '.jupyter',\n",
       " '.ms-ad',\n",
       " '.vscode',\n",
       " '1110',\n",
       " '3049109',\n",
       " '3D Objects',\n",
       " 'AppData',\n",
       " 'Application Data',\n",
       " 'Contacts',\n",
       " 'Cookies',\n",
       " 'Customer Contracts.csv',\n",
       " 'Customer Demo.csv',\n",
       " 'Customer Engagements.csv',\n",
       " 'datasets',\n",
       " 'Desktop',\n",
       " 'Documents',\n",
       " 'Downloads',\n",
       " 'edb_pgagent_pg15.exe',\n",
       " 'Favorites',\n",
       " 'IntelGraphicsProfiles',\n",
       " 'Links',\n",
       " 'Local Settings',\n",
       " 'Microsoft',\n",
       " 'MicrosoftEdgeBackups',\n",
       " 'Music',\n",
       " 'My Documents',\n",
       " 'NetHood',\n",
       " 'NTUSER.DAT',\n",
       " 'ntuser.dat.LOG1',\n",
       " 'ntuser.dat.LOG2',\n",
       " 'NTUSER.DAT{6633646e-9574-11ec-a3af-e513d6db25ea}.TM.blf',\n",
       " 'NTUSER.DAT{6633646e-9574-11ec-a3af-e513d6db25ea}.TMContainer00000000000000000001.regtrans-ms',\n",
       " 'NTUSER.DAT{6633646e-9574-11ec-a3af-e513d6db25ea}.TMContainer00000000000000000002.regtrans-ms',\n",
       " 'ntuser.ini',\n",
       " 'OneDrive',\n",
       " 'Pictures',\n",
       " 'PostgreSQL.ipynb',\n",
       " 'PrintHood',\n",
       " 'Recent',\n",
       " 'Saved Games',\n",
       " 'Searches',\n",
       " 'SendTo',\n",
       " 'Start Menu',\n",
       " 'Templates',\n",
       " 'Untitled.ipynb',\n",
       " 'Videos']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae8d7d1-66dd-4b65-bf44-cace1df53a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find CSV files in my current working directory #isolate only the CSV files\n",
    "\n",
    "csv_files = []\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d057af30-b618-477a-96c3-f282e587a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new directory\n",
    "dataset_dir = 'datasets'\n",
    "\n",
    "#create the bash command to make a new directory\n",
    "# mkdir dataset_dir\n",
    "try:\n",
    "    mkdir = 'mkdir {0}'.format(dataset_dir)\n",
    "    os.system(mkdir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae429bb1-e41f-4125-84dc-47a1ce6878a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved Customer Contracts.csv to datasets\\Customer Contracts.csv\n",
      "Moved Customer Demo.csv to datasets\\Customer Demo.csv\n",
      "Moved Customer Engagements.csv to datasets\\Customer Engagements.csv\n"
     ]
    }
   ],
   "source": [
    "#move the CSV files in the new directory\n",
    "import shutil\n",
    "#mv filename directory\n",
    "for csv in csv_files:\n",
    "    src_file = os.path.join(h, csv)  # Full path of the source file\n",
    "    dst_file = os.path.join(dataset_dir, csv)  # Full path of the destination file\n",
    "    shutil.move(src_file, dst_file)\n",
    "    print(f\"Moved {csv} to {dst_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c34002b-586b-4e6f-8e51-4709f1cf37b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Contracts.csv\n",
      "Customer Demo.csv\n",
      "Customer Engagements.csv\n"
     ]
    }
   ],
   "source": [
    "data_path = h+'/'+dataset_dir+'/'\n",
    "\n",
    "df = {}  # Use curly braces to initialize an empty dictionary instead of a list\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df[file] = pd.read_csv(data_path+file)\n",
    "    except UnicodeDecodeError:\n",
    "        df[file] = pd.read_csv(data_path+file, encoding=\"ISO-8859-1\")\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f071bc48-d416-47a7-b388-62ac4b9cfdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_contracts\n",
      "customer_name varchar, start_date varchar, end_date varchar, contract_amount_m float, invoice_sent varchar, paid varchar\n",
      "opened database successfully\n",
      "customer_contracts was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table customer_contracts imported to db completed\n",
      "customer_demo\n",
      "customer_id int, customer_name varchar, employee_count int, office_location varchar\n",
      "opened database successfully\n",
      "customer_demo was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table customer_demo imported to db completed\n",
      "customer_engagements\n",
      "customer_id int, num_of_users int, _of_all_employees varchar, sso varchar, launched varchar\n",
      "opened database successfully\n",
      "customer_engagements was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table customer_engagements imported to db completed\n",
      "all tables have been successfully imported into the db\n"
     ]
    }
   ],
   "source": [
    "for k in csv_files:\n",
    "    dataframe = df[k]\n",
    "    clean_tbl_name =  k.lower().replace(\" \",\"_\").replace(\"?\",\"\") \\\n",
    "                        .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\") \\\n",
    "                        .replace(\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\")\n",
    "\n",
    "    # remove.csv extension from clean_tbl_name\n",
    "    tbl_name = '{0}'.format(clean_tbl_name.split('.')[0])\n",
    "    print(tbl_name)\n",
    "\n",
    "    #clean table columns\n",
    "    dataframe.columns = [x.lower().replace(\" \",\"_\").replace(\"?\",\"\") \\\n",
    "                        .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\") \\\n",
    "                        .replace(\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\") for  x in dataframe.columns]\n",
    "    \n",
    "\n",
    "    #replacement dictionary that maps pandas dtypes to sql dtypes\n",
    "    replacements = {\n",
    "        'object': 'varchar', 'float64': 'float',\n",
    "        'int64': 'int',\n",
    "        'datetime64': 'timestamp',\n",
    "        'timedelta64 [ns]': 'varchar'\n",
    "    }\n",
    "    #table schema\n",
    "    col_str = \", \".join(\"{} {}\".format (n, d) for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(replacements))) \n",
    "    print(col_str)\n",
    "\n",
    "    #open a database connection\n",
    "\n",
    "    host = 'localhost'\n",
    "    dbname = 'importcsv'\n",
    "    username = 'postgres'\n",
    "    password = '1111'\n",
    "    \n",
    "    conn_string = \"host=%s dbname=%s user=%s password=%s\" % (host, dbname, username, password)\n",
    "    conn = psycopg2.connect (conn_string)\n",
    "    cursor = conn.cursor()\n",
    "    print('opened database successfully')\n",
    "\n",
    "    #drop table with same name \n",
    "    cursor.execute(\"drop table if exists %s;\" % (tbl_name))\n",
    "    \n",
    "    #create table\n",
    "    cursor.execute(\"create table %s (%s);\" % (tbl_name, col_str))\n",
    "    print('{0} was created successfully'.format(tbl_name))\n",
    "    \n",
    "    #insert values to table\n",
    "    \n",
    "    #save df to csv\n",
    "    dataframe.to_csv (k, header=dataframe.columns, index=False, encoding='utf-8')\n",
    "    \n",
    "    #open the csv file, save it as an object\n",
    "    my_file = open(k)\n",
    "    print('file opened in memory')\n",
    "\n",
    "    #upload to db\n",
    "    \n",
    "    SQL_STATEMENT = \"\"\"\n",
    "    COPY %s FROM STDIN WITH\n",
    "        CSV\n",
    "        HEADER\n",
    "        DELIMITER AS ','\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.copy_expert (sql=SQL_STATEMENT % tbl_name, file=my_file)\n",
    "    print('file copied to db')\n",
    "\n",
    "    cursor.execute(\"grant select on table %s to public\" % tbl_name)\n",
    "    conn.commit()\n",
    "    \n",
    "    cursor.close()\n",
    "    print('table {0} imported to db completed'.format(tbl_name))\n",
    "\n",
    "#for loop end message\n",
    "print('all tables have been successfully imported into the db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df6f75-7645-4ef6-98c5-f569f28bbdac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
