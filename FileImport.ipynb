{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3f4ce4-641e-4aaa-9efb-5c62fb1d7f16",
   "metadata": {},
   "source": [
    "# Automated .csv file import into a PostgreSQL database\n",
    "\n",
    "## Introduction :\n",
    "\n",
    "### In PostgreSQL, it is necessary to specify the data types of columns before importing data. This can be more cumbersome, especially when dealing with tables with a large number of columns. The requirement to define the data types in advance can make the import process more time-consuming and error-prone, as it involves accurately specifying the data types for each column.\n",
    "\n",
    "### However, it's important to note that PostgreSQL's approach of explicitly defining data types offers the advantage of data integrity enforcement and stricter control over the data being imported. It ensures that the imported data aligns with the defined table schema, reducing the risk of data inconsistencies or integrity violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e8afa4-0436-4248-af6b-9b49f7abadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3989d1a7-53a3-46cb-94ce-a6515dcb4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07851223-d45a-4434-b94f-bfa0d0bfd628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '.jupyter',\n",
       " '.vscode',\n",
       " 'build',\n",
       " 'CarPrice_Assignment.csv',\n",
       " 'confini.ipynb',\n",
       " 'CONVERT',\n",
       " 'convert.py',\n",
       " 'CSVfile_Folder',\n",
       " 'Customer Contracts$.csv',\n",
       " 'Customer Demo.csv',\n",
       " 'Customer Engagements.csv',\n",
       " 'datasets',\n",
       " 'dist',\n",
       " 'DLLs',\n",
       " 'Doc',\n",
       " 'etc',\n",
       " 'FileImport.ipynb',\n",
       " 'FINALKPEHLE.ipynb',\n",
       " 'House_Rent_Dataset.csv',\n",
       " 'h_config.ini',\n",
       " 'include',\n",
       " 'Lib',\n",
       " 'libs',\n",
       " 'LICENSE.txt',\n",
       " 'LinearRegression-Copy1.ipynb',\n",
       " 'LinearRegression-Copy2.ipynb',\n",
       " 'LinearRegression.ipynb',\n",
       " 'L_Python.ipynb',\n",
       " 'main.py',\n",
       " 'mlregression-house-rent-predictions.ipynb',\n",
       " 'MOSTIMP.py',\n",
       " 'NEWS.txt',\n",
       " 'output33.csv',\n",
       " 'popularityrent.ipynb',\n",
       " 'PostgreSQL copy.ipynb',\n",
       " 'PostgreSQL.ipynb',\n",
       " 'Project1.py',\n",
       " 'Project1.py.spec',\n",
       " 'Project1.spec',\n",
       " 'PROJECT1README.md',\n",
       " 'Project2.ipynb',\n",
       " 'Project2read.ipynb',\n",
       " 'python.exe',\n",
       " 'python3.dll',\n",
       " 'python311.dll',\n",
       " 'pythonw.exe',\n",
       " 'Scripts',\n",
       " 'share',\n",
       " 'statsfinal.csv',\n",
       " 'tcl',\n",
       " 'Tools',\n",
       " 'transformed_house_rent_data.csv',\n",
       " 'vcruntime140.dll',\n",
       " 'vcruntime140_1.dll']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f402d58-b7ff-49bd-a620-0e4aa05adbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find CSV files in my current working directory \n",
    "#isolate only the CSV files\n",
    "\n",
    "csv_files = []\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3703128b-98f2-4a96-8630-d689086d2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new directory\n",
    "dataset_dir = 'CSVfile_Folder'\n",
    "\n",
    "#create the bash command to make a new directory\n",
    "# mkdir dataset_dir\n",
    "try:\n",
    "    mkdir = 'mkdir {0}'.format(dataset_dir)\n",
    "    os.system(mkdir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b464cc-8ec8-4b96-b4d6-b11dd0dd912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved CarPrice_Assignment.csv to CSVfile_Folder\\CarPrice_Assignment.csv\n",
      "Moved Customer Contracts$.csv to CSVfile_Folder\\Customer Contracts$.csv\n",
      "Moved Customer Demo.csv to CSVfile_Folder\\Customer Demo.csv\n",
      "Moved Customer Engagements.csv to CSVfile_Folder\\Customer Engagements.csv\n",
      "Moved House_Rent_Dataset.csv to CSVfile_Folder\\House_Rent_Dataset.csv\n",
      "Moved output33.csv to CSVfile_Folder\\output33.csv\n",
      "Moved statsfinal.csv to CSVfile_Folder\\statsfinal.csv\n",
      "Moved transformed_house_rent_data.csv to CSVfile_Folder\\transformed_house_rent_data.csv\n"
     ]
    }
   ],
   "source": [
    "#move the CSV files in the new directory\n",
    "import shutil\n",
    "#mv filename directory\n",
    "for csv in csv_files:\n",
    "    src_file = os.path.join(h, csv)  # Full path of the source file\n",
    "    dst_file = os.path.join(dataset_dir, csv)  # Full path of the destination file\n",
    "    shutil.move(src_file, dst_file)\n",
    "    print(f\"Moved {csv} to {dst_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221a0531-400f-4ee3-8389-73432ad4a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarPrice_Assignment.csv\n",
      "Customer Contracts$.csv\n",
      "Customer Demo.csv\n",
      "Customer Engagements.csv\n",
      "House_Rent_Dataset.csv\n",
      "output33.csv\n",
      "statsfinal.csv\n",
      "transformed_house_rent_data.csv\n"
     ]
    }
   ],
   "source": [
    "data_path = h+'/'+dataset_dir+'/'\n",
    "\n",
    "df = {}  # Use curly braces to initialize an empty dictionary instead of a list\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df[file] = pd.read_csv(data_path+file)\n",
    "    except UnicodeDecodeError:\n",
    "        df[file] = pd.read_csv(data_path+file, encoding=\"UTF-8\")\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366e1420-f961-471d-88ce-4125f9493f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carprice_assignment\n",
      "car_id int, symboling int, carname varchar, fueltype varchar, aspiration varchar, doornumber varchar, carbody varchar, drivewheel varchar, enginelocation varchar, wheelbase float, carlength float, carwidth float, carheight float, curbweight int, enginetype varchar, cylindernumber varchar, enginesize int, fuelsystem varchar, boreratio float, stroke float, compressionratio float, horsepower int, peakrpm int, citympg int, highwaympg int, price float\n",
      "opened database successfully\n",
      "carprice_assignment was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table carprice_assignment imported to db completed\n",
      "customer_contracts\n",
      "customer_name varchar, start_date varchar, end_date varchar, contract_amount_m float, invoice_sent varchar, paid varchar\n",
      "opened database successfully\n",
      "customer_contracts was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table customer_contracts imported to db completed\n",
      "customer_demo\n",
      "customer_id int, customer_name varchar, employee_count int, office_location varchar\n",
      "opened database successfully\n",
      "customer_demo was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table customer_demo imported to db completed\n",
      "customer_engagements\n",
      "customer_id int, num_of_users int, _of_all_employees varchar, sso varchar, launched varchar\n",
      "opened database successfully\n",
      "customer_engagements was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table customer_engagements imported to db completed\n",
      "house_rent_dataset\n",
      "posted_on varchar, bhk int, rent int, size int, floor varchar, area_type varchar, area_locality varchar, city varchar, furnishing_status varchar, tenant_preferred varchar, bathroom int, point_of_contact varchar\n",
      "opened database successfully\n",
      "house_rent_dataset was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table house_rent_dataset imported to db completed\n",
      "output33\n",
      "bhk int, rent int, size int, enfloorlevel float, en_total_floors float, en_area_type int, en_city int, encoded_furnish_type float, en_tenant_preferred int, bathroom int, en_ptofcontact float\n",
      "opened database successfully\n",
      "output33 was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table output33 imported to db completed\n",
      "statsfinal\n",
      "_ int, date varchar, q_p1 int, q_p2 int, q_p3 int, q_p4 int, s_p1 float, s_p2 float, s_p3 float, s_p4 float\n",
      "opened database successfully\n",
      "statsfinal was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table statsfinal imported to db completed\n",
      "transformed_house_rent_data\n",
      "original_data float, transformed_data float\n",
      "opened database successfully\n",
      "transformed_house_rent_data was created successfully\n",
      "file opened in memory\n",
      "file copied to db\n",
      "table transformed_house_rent_data imported to db completed\n",
      "all tables have been successfully imported into the db\n"
     ]
    }
   ],
   "source": [
    "for k in csv_files:\n",
    "    dataframe = df[k]\n",
    "    clean_tbl_name =  k.lower().replace(\" \",\"_\").replace(\"?\",\"\") \\\n",
    "                        .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\") \\\n",
    "                        .replace(\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\")\n",
    "\n",
    "    # remove.csv extension from clean_tbl_name\n",
    "    tbl_name = '{0}'.format(clean_tbl_name.split('.')[0])\n",
    "    print(tbl_name)\n",
    "\n",
    "    #clean table columns\n",
    "    dataframe.columns = [x.lower().replace(\" \",\"_\").replace(\"?\",\"\") \\\n",
    "                        .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\") \\\n",
    "                        .replace(\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\") for  x in dataframe.columns]\n",
    "    \n",
    "\n",
    "    #replacement dictionary that maps pandas dtypes to sql dtypes\n",
    "    replacements = {\n",
    "        'object' : 'varchar', 'float64' : 'float',\n",
    "        'int64' : 'int',\n",
    "        'datetime64' : 'timestamp',\n",
    "        'timedelta64 [ns]' : 'varchar'\n",
    "    }\n",
    "    #table schema\n",
    "    col_str = \", \".join(\"{} {}\".format (n, d) for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(replacements))) \n",
    "    print(col_str)\n",
    "\n",
    "   \n",
    "    #To connect with database\n",
    "    \n",
    "    # Read configuration file\n",
    "    import configparser\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('h_config.ini')\n",
    "    \n",
    "    # Retrieve database credentials\n",
    "    username = config.get('postgresql', 'username')\n",
    "    password = config.get('postgresql', 'password')\n",
    "    host = config.get('postgresql', 'host')\n",
    "    dbname = config.get('postgresql', 'dbname')\n",
    "  \n",
    "    conn = psycopg2.connect(\n",
    "        user=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        database=dbname\n",
    "    )\n",
    "    \n",
    "    # Your code to work with the database goes here\n",
    "    \n",
    "    \n",
    "    # conn_string = \"host=%s dbname=%s user=%s password=%s\" % (host, dbname, username, password)\n",
    "    # conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "    print('opened database successfully')\n",
    "\n",
    "    #drop table with same name \n",
    "    cursor.execute(\"drop table if exists %s;\" % (tbl_name))\n",
    "    \n",
    "    #create table\n",
    "    query = 'CREATE TABLE \"%s\" (%s);'\n",
    "    cursor.execute(query % (tbl_name, col_str))\n",
    "\n",
    "    #cursor.execute(\"create table %s (%s);\" % (tbl_name, col_str))\n",
    "    print('{0} was created successfully'.format(tbl_name))\n",
    "    \n",
    "    #insert values to table\n",
    "    \n",
    "    #save df to csv\n",
    "    dataframe.to_csv (k, header=dataframe.columns, index=False, encoding='UTF-8')\n",
    "    \n",
    "    #open the csv file, save it as an object\n",
    "    my_file = open(k)\n",
    "    print('file opened in memory')\n",
    "\n",
    "    #upload to db\n",
    "    \n",
    "    SQL_STATEMENT = \"\"\"\n",
    "    COPY %s FROM STDIN WITH\n",
    "        CSV\n",
    "        HEADER\n",
    "        DELIMITER AS ','\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.copy_expert (sql=SQL_STATEMENT % tbl_name, file=my_file)\n",
    "    print('file copied to db')\n",
    "\n",
    "    cursor.execute(\"grant select on table %s to public\" % tbl_name)\n",
    "    conn.commit()\n",
    "    \n",
    "    cursor.close()\n",
    "    print('table {0} imported to db completed'.format(tbl_name))\n",
    "\n",
    "#for loop end message\n",
    "print('all tables have been successfully imported into the db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f7f786-6eeb-481e-af5f-38ab6f9ab9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PostgreSQL, column names cannot contain certain special characters, such as colons (\":\"). The error is caused by the colon in the column name \"unnamed:_0\".\n",
    "#To fix this issue, you should choose a valid column name that follows the naming rules:\n",
    "\n",
    "#1.Column names must start with a letter or an underscore (_).\n",
    "#2.Subsequent characters can include letters, digits, and underscores.\n",
    "#3.Avoid using special characters or spaces in column names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
